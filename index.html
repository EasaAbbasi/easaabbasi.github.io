<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Easa AliAbbasi</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
</head>

<body>

<!-- DARK MODE BUTTON -->
<button id="theme-toggle">Dark mode</button>

<div class="container">

    <!-- LEFT COLUMN: STICKY TABS -->
    <aside class="column-left">
        <nav class="vertical-nav">
            <a href="#about">About</a>
            <a href="#education">Education</a>
            <a href="#research">Research</a>
            <a href="#publications">Publications</a>
            <a href="#projects">Projects</a>
            <a href="#cv">CV</a>
        </nav>
    </aside>

    <!-- MIDDLE COLUMN: MAIN CONTENT -->
    <main class="column-main">

        <!-- PROFILE PHOTO AND INFO -->
        <div class="profile">
            <img src="profile.jpg" alt="Easa Abbasi" class="profile-photo">
            <h1>Easa AliAbbasi</h1>
            <p class="subtitle">Postdoctoral Researcher | Max Planck Institute for Informatics</p>
            <p class="links">
                <a href="https://scholar.google.com/citations?user=4ai61x8AAAAJ&hl=en" target="_blank">Google Scholar</a> ·
                <a href="https://orcid.org/0000-0002-2443-8416" target="_blank">ORCID</a> ·
                <a href="https://github.com/EasaAbbasi" target="_blank">GitHub</a>
                <a href="https://www.youtube.com/@EasaAliAbbasi" target="_blank">YouTube</a>
                
            </p>
            <p>Email: easa.aliabbasi[at]mpi-inf.mpg.de</p>
        </div>

        <section id="about">
            <h2>About</h2>
            <p>
                I am a postdoctoral researcher at the <a href="https://www.mpi-inf.mpg.de/home">Max Planck Institute for Informatics</a> and a member
                of the <a href="https://sensint.mpi-inf.mpg.de/">Sensorimotor Interaction Group</a>. I received my Ph.D. degree from Koc University in
                January 2024. During my Ph.D. studies at the <a href="https://rml.ku.edu.tr/">Robotics and Mechatronics Laboratory (RML)</a>, I worked
                on characterizing electroadhesive tactile displays and understanding the human sense of touch. My current research interests include
                haptics, human-computer interaction (HCI), and the intersection of haptics and AI. In my research, I aim to develop methods and tools
                for understanding human sense of touch, developing AI models to enhance haptics, and developing tactile displays to convey sensory information.
            </p>
        </section>

        <section id="education">
            <h2>Education</h2>
            <ul>
                <li><strong>PhD</strong>, Computational Sciences and Engineering — Koc University, 01/2024</li>
                <li><strong>MSc</strong>, Mechatronics Engineering — University of Tabriz, 02/2017</li>
                <li><strong>BSc</strong>, Electrical and Electronics Engineering — zad University, 08/2014</li>
            </ul>
        </section>
        
        <section id="research">
            <h2>Research Interests</h2>
            <ul>
                <li>Haptics</li>
                <li>Human-Computer Interaction (HCI)</li>
                <li>Intersection of Haptics and AI</li>
            </ul>
        </section>

        <section id="publications">
            <h2>Selected Publications</h2>
            <ol>
                <li>
                    <strong>E. AliAbbasi</strong>, D. Wittchen, Y. Li, S. Lu, T. Müller, D. Degraen, T. Leimkühler, S. H. Yoon, H. Seifi, O. Schneider, H. Culbertson, J. Steimle, and P. Strohmeier, <em>AI for Haptics and Haptics for AI: Challenges and Opportunities</em>,
                    Extended Abstracts of the 2026 CHI Conference on Human Factors in Computing Systems (CHI EA’26), Barcelona, Spain, Association for Computing Machinery, 2026.
                    <br>
                    
                    <a href="AliAbbasi_2026_AIforHapticsHapticsforAI.pdf" target="_blank">[PDF]</a>
                    ·
                    <a href="#" class="bib-link" data-bib-id="bib-AliAbbasi26CHI">[BibTeX]</a>
                
                    <!-- HIDDEN BIBTEX -->
                    <pre id="bib-AliAbbasi26CHI" class="bibtex-content">
                    @inproceedings{aliabbasi2026ai,
                      title     = {AI for Haptics and Haptics for AI: Challenges and Opportunities},
                      author    = {AliAbbasi, Easa and Wittchen, Dannis and Li, Yinan and Lu, Shihan and Müller, Thomas and Degraen, Donald and Leimkühler, Thomas and Yoon, Sang Ho and Seifi, Hasti and Schneider, Oliver and Culbertson, Heather and Steimle, Jürgen and Strohmeier, Paul},
                      booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA’26)},
                      abstract  = {AI has transformed methods and knowledge across many domains. However, the intersection of AI and haptics remains underexplored. While modern AI techniques-fueled by machine learning and using powerful techniques such as generative modeling and reinforcement learning-offer powerful opportunities for advancing haptic design, insights from haptics research, such as perception modeling and adaptive interaction-grounded in human touch, embodiment , and multisensory integration-can also play a critical role in shaping more human-centered AI systems. This workshop will bring together an interdisciplinary community of researchers from HCI, haptics, AI, robotics, and design to (1) identify pressing questions in haptics that could benefit from AI approaches and (2) highlight ways in which haptic knowledge can support the development of embodied and context-aware AI. Through position papers and paper presentations, we will map key challenges, exchange methods, and explore new research directions that connect the two fields. By framing haptics and AI as mutually reinforcing, the workshop aims to build a shared research agenda and foster collaborations that advance both the science of touch and the design of intelligent interactive systems.},
                      year      = {2026},
                      publisher = {Association for Computing Machinery},
                      address   = {Barcelona, Spain}
                    }
                    </pre>
                </li>

                <li>
                    A. Jingu, <strong>E. AliAbbasi</strong>, P. Strohmeier, J. Steimle. “Scene2Hap: Combining LLMs and physical modeling for automatically generating vibrotactile signals for full VR scenes”, <em>arXiv preprint</em>, arXiv:2504.19611, 2025.
                    <br>
                    
                    <a href="Jingu_2025_Scene2Hap.pdf" target="_blank">[PDF]</a>
                    ·
                    <a href="#" class="bib-link" data-bib-id="bib-Jingu26CHI">[BibTeX]</a>
                
                    <!-- HIDDEN BIBTEX -->
                    <pre id="bib-Jingu26CHI" class="bibtex-content">
                    @article{jingu2026scene2hap,
                      title     = {Scene2Hap: Combining LLMs and physical modeling for automatically generating vibrotactile signals for full VR scenes},
                      author    = {Jingu, Arata and AliAbbasi, Easa and Steimle, Jürgen and Strohmeier, Paul},
                      booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA’26)},
                      abstract  = {Haptic feedback contributes to immersive virtual reality (VR) experiences. Designing such feedback at scale, for all objects within a VR scene and their respective arrangements, remains a time-consuming task. We present Scene2Hap, an LLM-centered system that automatically designs object-level vibrotactile feedback for entire VR scenes based on the objects' semantic attributes and physical context. Scene2Hap employs a multimodal large language model to estimate the semantics and physical context of each object, including its material properties and vibration behavior, from the multimodal information present in the VR scene. This semantic and physical context is then used to create plausible vibrotactile signals by generating or retrieving audio signals and converting them to vibrotactile signals. For the more realistic spatial rendering of haptics in VR, Scene2Hap estimates the propagation and attenuation of vibration signals from their source across objects in the scene, considering the estimated material properties and physical context, such as the distance and contact between virtual objects. Results from two user studies confirm that Scene2Hap successfully estimates the semantics and physical context of VR scenes, and the physical modeling of vibration propagation improves usability, perceived materiality, and spatial awareness.},
                      year      = {2025},
                      publisher = {arXiv preprint},
                    }
                    </pre>
                </li>

                <li>
                    <strong>E. AliAbbasi</strong>, N. Sabnis, Y. Ding, N. Wagener, and P. Strohmeier, “Haptic redirection: Modulating hand movement speed with vibrotactile feedback”, <em>Proceedings of the Mensch und Computer 2025 (MuC ’25)</em>, Chemnitz, Germany, Association for Computing Machinery, 631–636, 2025.
                    <br>
                    
                    <a href="AliAbbasi_2025_HapticRedirection.pdf" target="_blank">[PDF]</a>
                    ·
                    <a href="#" class="bib-link" data-bib-id="bib-AliAbbasi25MUC">[BibTeX]</a>
                
                    <!-- HIDDEN BIBTEX -->
                    <pre id="bib-AliAbbasi25MUC" class="bibtex-content">
                    @inproceedings{aliabbasi2025haptic_redirection,
                    author = {AliAbbasi, Easa and Sabnis, Nihar and Ding, Yuran and Wagener, Nadine and Strohmeier, Paul},
                    title = {Haptic Redirection: Modulating Hand Movement Speed with Vibrotactile Feedback},
                    year = {2025},
                    isbn = {9798400715822},
                    publisher = {Association for Computing Machinery},
                    address = {New York, NY, USA},
                    url = {https://doi.org/10.1145/3743049.3748585},
                    doi = {10.1145/3743049.3748585},
                    abstract = {Redirecting user movement in Virtual Reality (VR) can expand perceived virtual space while accommodating limited physical space. Existing methods primarily rely on visual and auditory cues. This work explores the foundation for an alternative approach using haptic cues. We were inspired by the observation that vibrations arise when a finger moves over a textured surface, influenced by two factors: the scanning speed and the surface properties. While prior research has focused on using vibrations to modify texture perception, we investigate the second factor; modifying vibrations to influence movement speed. Through three psychophysical experiments, we show that: (1) Human movement speed is affected by the properties of vibrotactile feedback. (2) Movement speed remains unchanged during transitions if users are aware of vibrotactile feedback changes. However, we found that (3) movement speed increases by ∼ 20\% when vibration pulses are reduced by 50\%, provided users are unaware of the vibrotactile feedback change.},
                    booktitle = {Proceedings of the Mensch Und Computer 2025},
                    pages = {631–636},
                    numpages = {6},
                    keywords = {Hand Redirection, Virtual Reality, Vibrotactile Haptics, Perception, Sensorimotor Contingencies},
                    location = {Chemnitz, Germany},
                    series = {MuC '25}
                    }
                    </pre>
                </li>

                <li>
                    <strong>E. AliAbbasi</strong>, G. Vega, D. Wittchen, and P. Strohmeier, “Physical compliance and the compliance illusion: The importance of action for perception”, IEEE World Haptic Conference (WHC25), Suwon, Korea, IEEE, 2025.
                    <br>
                    
                    <a href="AliAbbasi_2025_PhysicalCompliance.pdf" target="_blank">[PDF]</a>
                    ·
                    <a href="#" class="bib-link" data-bib-id="bib-AliAbbasi25WHC">[BibTeX]</a>
                
                    <!-- HIDDEN BIBTEX -->
                    <pre id="bib-AliAbbasi25WHC" class="bibtex-content">
                    @inproceedings{aliabbasi2025physical_compliance,
                    author={AliAbbasi, Easa and Vega, Gabriela and Wittchen, Dennis and Strohmeier, Paul},
                    booktitle={2025 IEEE World Haptics Conference (WHC)}, 
                    title={Physical Compliance and the Compliance Illusion: The Importance of Action for Perception}, 
                    year={2025},
                    pages={349-356},
                    abstract={The perception of an object's compliance can be manipuated using the grain-based vibrotactile compliance illusion. Despite the growing interest in creating virtual compliance using this method, its perceptual mechanism is poorly understood. To address this gap in knowledge, we present a detailed analysis of compliance estimates and pressure profiles of exploration behaviors of 12 participants while perceiving both physical and virtual compliance. Our results indicate that the experience of virtual compliance provided by the compliance illusion is distinct from that of physical compliance and that these experiences are mediated by distinct sensorimotor processes. This is evident in the non-additive nature of both real and illusory compliance perceptions and the separable exploratory actions of the participants in response to real and illusory compliance. These insights affect the design of augmented and virtual tactile reality systems, shed light on the mechanisms of compliance illusion, and provide data in support of closed-loop theories of tactile perception.},
                    location={Suwon, Korea},
                    publisher={IEEE},
                    doi={10.1109/WHC64065.2025.11123193}
                    }
                    </pre>
                </li>

                <li>
                    P. Strohmeier, L. Turmo Vidal, G. Vega, C. N. Reed, A. Mazursky, <strong>E. AliAbbasi</strong>, A. Tajadura-Jimenez, and J. Steimle, “Sensorimotor devices: Coupling sensing and actuation to augment bodily experience”, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA’25), Yokohama, Japan, Association for Computing Machinery, 2025.
                    <br>
                    
                    <a href="Strohmeier_2025_SensorimotorDevices.pdf" target="_blank">[PDF]</a>
                    ·
                    <a href="#" class="bib-link" data-bib-id="bib-Strohmeier25CHI">[BibTeX]</a>
                
                    <!-- HIDDEN BIBTEX -->
                    <pre id="bib-Strohmeier25CHI" class="bibtex-content">
                    @inproceedings{strohmeier2025sensorimotor,
                    author = {Strohmeier, Paul and Turmo Vidal, Laia and Vega, Gabriela and Reed, Courtney N. and Mazursky, Alex and AliAbbasi, Easa and Tajadura-Jim\'{e}nez, Ana and Steimle, J\"{u}rgen},
                    title = {Sensorimotor Devices: Coupling Sensing and Actuation to Augment Bodily Experience},
                    year = {2025},
                    isbn = {9798400713958},
                    publisher = {Association for Computing Machinery},
                    address = {New York, NY, USA},
                    url = {https://doi.org/10.1145/3706599.3706735},
                    doi = {10.1145/3706599.3706735},
                    abstract = {An emerging space in interface research is wearable devices that closely couple their sensing and actuation abilities. A well-known example is MetaLimbs [39], where sensed movements of the foot are directly mapped to the actuation of supernumerary robotic limbs. These systems are different from wearables focused on sensing, such as fitness trackers, or wearables focused on actuation, such as VR headsets. They are characterized by tight coupling between the user’s action and the resulting digital feedback from the device, in time, space, and mode. The properties of this coupling are critical for the user’s experience, including the user’s sense of agency, body ownership, and experience of the surrounding world. Understanding such systems is an open challenge, which requires knowledge not only of computer science and HCI, but also Psychology, Physiology, Design, Engineering, Cognitive Neuroscience, and Control Theory. This workshop aims to foster discussion between these diverse disciplines and to identify links and synergies in their work, ultimately developing a common understanding of future research directions for systems that intrinsically couple sensing and action.},
                    booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
                    articleno = {799},
                    numpages = {7},
                    keywords = {sensorimotor interaction, wearables, feedback systems, motion-coupled feedback},
                    location = {Yokohama, Japan},
                    series = {CHI EA '25}
                    }
                    </pre>
                </li>
                
            </ol>
            <p>
                Full list available on
                <a href="https://scholar.google.com/citations?user=4ai61x8AAAAJ&hl=en" target="_blank">Google Scholar</a>.
            </p>
        </section>
        
        <section id="projects">
            <h2>Projects</h2>
            <div class="project-grid">
                <div class="project">
                    <img src="CHI26.jpg" alt="Project figure CHI26">
                    <h3>Scene2Hap: Combining LLMs and Physical Modeling for Automatically Generating Vibrotactile Signals for Full VR Scenes</h3>
                    <p>We present Scene2Hap, an LLM-centered system that automatically designs object-level vibrotactile feedback for entire VR scenes based on the objects' semantic attributes and physical context. Scene2Hap employs a multimodal large language model to estimate the semantics and physical context of each object, including
                        its material properties and vibration behavior, from the multimodal information present in the VR scene. This semantic and physical context is then used to create plausible vibrotactile signals by generating or retrieving audio signals and converting them to vibrotactile signals. For the more realistic spatial
                        rendering of haptics in VR, Scene2Hap estimates the propagation and attenuation of vibration signals from their source across objects in the scene, considering the estimated material properties and physical context, such as the distance and contact between virtual objects. Results from two user studies confirm that
                        Scene2Hap successfully estimates the semantics and physical context of VR scenes, and the physical modeling of vibration propagation improves usability, perceived materiality, and spatial awareness.</p>
                    <p>This project resulted in a publication in <a href="https://arxiv.org/abs/2504.19611">arXiv'25.</a></p>
                </div>
                <div class="project">
                    <img src="WHC25.jpg" alt="Project figure WHC25">
                    <h3>Physical Compliance and the Compliance Illusion: The Importance of Action for Perception</h3>
                    <p>We present a detailed analysis of compliance estimates and pressure profiles of exploration behaviors of 12 participants while perceiving both physical and virtual compliance. Our results indicate that the experience of virtual compliance provided by the compliance illusion is distinct from that of physical compliance
                        and that these experiences are mediated by distinct sensorimotor processes. This is evident in the non-additive nature of both real and illusory compliance perceptions and the separable exploratory actions of the participants in response to real and illusory compliance. These insights affect the design of augmented
                        and virtual tactile reality systems, shed light on the mechanisms of compliance illusion, and provide data in support of closed-loop theories of tactile perception.</p>
                    <p>This project resulted in a publication in <a href="https://ieeexplore.ieee.org/abstract/document/11123193">WHC'25.</a></p>
                </div>
                <div class="project">
                    <img src="ToH25.jpg" alt="Project figure ToH25">
                    <h3>Effect of Finger Orientation on Contact Stiffness and Area During Sliding</h3>
                    <p>We conducted an experimental study to investigate the evolution of apparent contact area between a human fingerpad and a smooth flat surface under normal loading (stationary finger) and combined loading (sliding finger) conditions for 4 different internal rotations of the index finger (away from the second finger)
                        about its axial (longitudinal) axis and 2 different sliding directions. Our results show a reduction in the contact area for radial sliding as expected, but a surprising increase in the ulnar direction for the higher finger rotations. We argue that this asymmetric behavior in contact area evolution stems from the
                        changes in the equivalent radius of curvature and stiffening of the finger as the rotation angle increases, which manifests itself as the asymmetric stress distribution at the leading and trailing edges of the fingerpad in our finite element simulations.</p>
                    <p>This project resulted in a publication in <a href="https://ieeexplore.ieee.org/document/10778980">ToH'25.</a></p>
                </div>
                <div class="project">
                    <img src="AIS24.jpg" alt="Project figure AIS24">
                    <h3>Experimental Estimation of Gap Thickness and Electrostatic Forces Between Contacting Objects Under Electroadhesion.</h3>
                    <p>We proposed a new and systematic approach based on electrical impedance measurements to infer the electrostatic forces between two dielectric materials under electroadhesion (EA). The proposed approach is applied to tactile displays, where skin and voltage-induced touchscreen impedances are measured and
                        subtracted from the total impedance to obtain the remaining impedance to estimate the electrostatic forces between the finger and the touchscreen. This approach also marks the first instance of experimental estimation of the average air gap thickness between a human finger and a voltage-induced
                        capacitive touchscreen. Moreover, the effect of electrode polarization impedance on EA is investigated. Precise measurements of electrical impedances confirm that electrode polarization impedance exists in parallel with the impedance of the air gap, particularly at low frequencies, giving rise to
                        the commonly observed charge leakage phenomenon in EA.</p>
                    <p>This project resulted in a publication in <a href="https://advanced.onlinelibrary.wiley.com/doi/10.1002/aisy.202300618">Advanced Intelligentt Systems'24.</a></p>
                    <p><a href="https://www.youtube.com/watch?v=nX8x39G3IdU&t=23s">Video</a></p>
                </div>
                <div class="project">
                    <img src="ASR24.jpg" alt="Project figure ASR24">
                    <h3>Electronic Skin (E-Skin)</h3>
                    <p>Inspired by the sensory properties of skin, we presented a micro-fabricated, multiplexed electronic skin (e-skin) with actuators for sensory feedback in upper limb amputation. The piezoelectric-capacitive sensor array detects static pressure, temperature, vibration, and texture, with
                        integrated actuators stimulating the skin to provide real-time feedback. The sensors integrate with actuators via readout electronics, making the system standalone and easy to use. The flexible, compact sensor array design (two pixels within a 1 cm² footprint) detects a wide range of
                        pressure (0.5–10 kPa), temperature (22–60 °C), vibration (35–100 Hz), and texture (2.5–45 Hz), suitable for daily use. The e-skin, attached to a prosthetic finger, is tested for feasibility on human volunteers with wrist-mounted actuators.</p>
                    <p>This project resulted in a publication in <a href="https://advanced.onlinelibrary.wiley.com/doi/10.1002/adsr.202400100">Advanced Sensor Research'24.</a></p>
                </div>
                <div class="project">
                    <img src="ToH24.jpg" alt="Project figure ToH24">
                    <h3>Effect of Finger Moisture on Tactile Perception of Electroadhesion</h3>
                    <p>We investigated the effect of finger moisture on the tactile perception of electroadhesion with 10 participants. Participants with moist fingers exhibited markedly higher threshold levels. Our electrical impedance measurements showed a substantial reduction in impedance magnitude
                        when sweat was present at the finger-touchscreen interface, indicating increased conductivity. Supporting this, our mechanical friction measurements showed that the relative increase in electrostatic force due to electroadhesion is lower for a moist finger.</p>
                    <p>This project resulted in a publication in <a href="https://ieeexplore.ieee.org/document/10637769">ToH'24.</a></p>
                </div>
                <div class="project">
                    <img src="WHC23.jpg" alt="Project figure WHC23">
                    <h3>Effect of Electrode Polarization Impedance on Electroadhesion</h3>
                    <p>We investigated the effect of electrode polarization (EP) impedance on electroadhesion (EA) between a human finger and a voltage-induced touchscreen. By conducting precise measurements of electrical impedances, we could ascertain the presence of EP impedance that operates in
                        parallel with the impedance of the air gap between finger and touchscreen. Our findings indicate that the EP impedance plays a dominant role, particularly at low frequencies, thereby giving rise to the charge leakage phenomenon commonly observed in EA.</p>
                    <p>This project resulted in a publication in <a href="https://2023.worldhaptics.org/wp-content/uploads/2023/06/1075-doc.pdf">WHC'23.</a></p>
                </div>
                <div class="project">
                    <img src="ToH23.jpg" alt="Project figure ToH23">
                    <h3>Tactile Perception of Coated Smooth Surfaces</h3>
                    <p>In this study, we first perform 2AFC experiments with 8 participants to quantify their tactile discrimination ability of 5 smooth glass surfaces coated with 3 different materials. We then measure the coefficient of friction between human finger and those 5 surfaces via a
                        custom-made tribometer and their surface energies via Sessile drop test performed with 4 different liquids. The results of our psychophysical experiments and the physical measurements show that coating material has a strong influence on our tactile perception and human
                        finger is capable of detecting differences in surface chemistry due to, possibly, molecular interactions.</p>
                    <p>This project resulted in a publication in <a href="https://ieeexplore.ieee.org/document/10128728">ToH'23.</a></p>
                    <p><a href="https://www.youtube.com/watch?v=QxiNxmBNQMo">Video</a></p>
                </div>
                <div class="project">
                    <img src="HX23.jpg" alt="Project figure HX23">
                    <h3>Fiber Winding Machine for Blood Oxygenator Prototypes</h3>
                    <p>In this project, a hollow-fiber assembly system is designed and built for winding research grade extracorporeal blood oxygenator mandrels at different layout dimensions so that these different configurations can be evaluated for mass transfer capacity and blood damage.
                        The hardware design and manufacturing details of this system presented together with its impact on the prototype oxygenator device assembly process.</p>
                    <p>This project resulted in a publication in <a href="https://www.sciencedirect.com/science/article/pii/S246806722300041X">HardwareX'23.</a></p>
                    <p><a href="https://www.youtube.com/watch?v=XAJHelPmr9U&t=1s">Video</a></p>
                </div>
                <div class="project">
                    <img src="ToH22.jpg" alt="Project figure ToH22">
                    <h3>Frequency-Dependent Electroadhesion</h3>
                    <p>In this project, we investigated the frequency-dependent behavior of electrostatic forces between a human finger and a voltage-induced touchscreen. The experimental data (collected using our custom-made tribometer) showed that electrostatic force follows an inverted parabolic
                        curve with a peak value around 250 Hz. Following the experimental characterization of electrostatic forces, an electro-mechanical model based on the fundamental laws of electric fields and Persson's multi-scale contact mechanics theory was developed. Compared to the existing
                        ones in the literature, the proposed model takes into account the charge accumulation and transfer at the interfaces of finger and touch screen. The model is in good agreement with the experimental data and shows that the change in magnitude of electrostatic force is mainly
                        due to the leakage of charge from the Stratum Corneum to the touch screen at frequencies lower than 250 Hz and electrical properties of the SC at frequencies higher than 250 Hz.</p>
                    <p>This project resulted in a publication in <a href="https://ieeexplore.ieee.org/document/9714802">ToH'22.</a></p>
                </div>
                <div class="project">
                    <img src="JEMAT22.jpg" alt="Project figure JEMAT22">
                    <h3>Vibration Analysis of MEMS Piezoelectric Cantilever Beam</h3>
                    <p>In a series of papers, we presented the mathematical modeling to analyse the vibrational behavior of MEMS piezoelectric cantilever beams.</p>
                    <p>This project resulted in a several publications in <a href="https://www.jemat.org/article_102586_970ffd48baa47fef0955d09e4f08a1fb.pdf">JEMT'20</a>, <a href="https://www.researchgate.net/profile/Easa-Aliabbasi/publication/338108487_Energy_Harvesting_Using_MEMS_Porous_Functionally_Graded_Piezoelectric_Cantilever_Beam/links/5dff347ea6fdcc2837352cc3/Energy-Harvesting-Using-MEMS-Porous-Functionally-Graded-Piezoelectric-Cantilever-Beam.pdf">ISAV'19</a></p>, <a href="https://d1wqtxts1xzle7.cloudfront.net/90987251/10008043.pdf?1738538499=&response-content-disposition=inline%3B+filename%3DDesign_And_Analysis_Of_A_Piezoelectric_B.pdf&Expires=1766415941&Signature=eoV6yOjGZ8bZvAIMUKlI5volWrYYx54vfm-rfN5phzOAuYDjKSUEFhvpsbIynsuJNIJGmMxdt8tzOAV2tA1FaUIrD4b-qQOcLi-X5yiXZ26sgJJA7k45qsHwfjfBjZoZacmnf7st2sV-T8BzZeJNb1M0fTqvjwyQTzoTkKRwv7z91JZUdxaYqWsmiN7eUyfaZ7NohkQT-Y94auEZPFXcwQ~LWckZwKhTG2nfIHhX94PIiBgOqbupOH-ADMYdz3jhqi3rpJ7lTYlGFnPLC0XpIiRavNUM~Xk5WiGdu-DDoq3tVWUVF5WS00aDP2AN7QpPmtreeTXF5jLVNQ3YzZ-Egw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">WASET'17</a>,
                    and ISAV'16.</p>
                </div>
                <div class="project">
                    <img src="KBEI25.jpg" alt="Project figure KBEI25">
                    <h3>Applying of PID, FPID, TID, and ITID controllers on AVR system using particle swarm optimization (PSO)</h3>
                    <p>We presented four PID, FPID, TID, and ITID controllers and applied each one on four different error functions in an AVR system by means of PSO algorithm. According to rising time, settling time and overshoot percentage, we prepared an understanding of finding the best controller easily while working with different error functions.</p>
                    <p>This project resulted in a several publications in <a href="https://ieeexplore.ieee.org/abstract/document/7436157">KBEI'15.</a></p>
                </div>
            </div>
        </section>

        <section id="cv">
            <h2>Curriculum Vitae</h2>
            <p><a href="cv.pdf" target="_blank">Download CV (PDF)</a></p>
        </section>

    </main>

    <!-- RIGHT COLUMN: NEWS -->
    <aside class="column-right">
        <div class="news">
            <h3>News & Updates</h3>
            <ul>
                <li>Jan 2025: Paper accepted at CHI 2026.</li>
                <li>Dec 2024: Joined Max Planck Institute for Informatics.</li>
                <li>Sep 2024: Published Scene2Hap preprint on arXiv.</li>
            </ul>
        </div>
    </aside>

</div>

<!-- BibTeX Modal -->
<div id="bibtex-modal" class="bibtex-modal">
    <div class="bibtex-box">
        <button id="bibtex-close" aria-label="Close BibTeX">×</button>
        <pre id="bibtex-text"></pre>
        <button id="bibtex-copy">Copy BibTeX</button>
    </div>
</div>

<script>
document.addEventListener("DOMContentLoaded", function () {

    /* ---------- DARK MODE ---------- */
    const themeToggle = document.getElementById("theme-toggle");
    themeToggle.addEventListener("click", function () {
        document.body.classList.toggle("dark");
        this.textContent = document.body.classList.contains("dark")
            ? "Light mode"
            : "Dark mode";
    });

    /* ---------- BIBTEX MODAL ---------- */
    const modal = document.getElementById("bibtex-modal");
    const modalText = document.getElementById("bibtex-text");
    const closeBtn = document.getElementById("bibtex-close");
    const copyBtn = document.getElementById("bibtex-copy");

    document.querySelectorAll(".bib-link").forEach(link => {
        link.addEventListener("click", function (e) {
            e.preventDefault();
            const bibId = this.dataset.bibId;
            modalText.textContent = document.getElementById(bibId).textContent;
            modal.style.display = "block";
        });
    });

    /* CLOSE MODAL */
    closeBtn.addEventListener("click", function () {
        modal.style.display = "none";
    });

    modal.addEventListener("click", function (e) {
        if (e.target === modal) {
            modal.style.display = "none";
        }
    });

    /* COPY BIBTEX (WITH FALLBACK) */
    copyBtn.addEventListener("click", function () {
        const text = modalText.textContent;

        if (navigator.clipboard) {
            navigator.clipboard.writeText(text).then(() => {
                copyBtn.textContent = "Copied!";
            });
        } else {
            const temp = document.createElement("textarea");
            temp.value = text;
            document.body.appendChild(temp);
            temp.select();
            document.execCommand("copy");
            document.body.removeChild(temp);
            copyBtn.textContent = "Copied!";
        }

        setTimeout(() => {
            copyBtn.textContent = "Copy BibTeX";
        }, 1500);
    });

});
</script>

</body>
</html>
